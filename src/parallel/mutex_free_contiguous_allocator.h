//
// Created by Chen, WenTao on 2025/9/9.
// Generated by Gemini.
//

#pragma once

#include <atomic>
#include <span>

template <bool EnableInfiniteLoop = false>
class MutexFreeContiguousAllocator {
 public:
  using Buffer = std::span<const char>;

  MutexFreeContiguousAllocator() = default;

  explicit MutexFreeContiguousAllocator(const Buffer buffer) : full_buffer_(buffer), offset_(0) {}

  MutexFreeContiguousAllocator(MutexFreeContiguousAllocator&& other) noexcept {
    if (this != &other) {
      *this = std::move(other);
    }
  }

  MutexFreeContiguousAllocator& operator=(MutexFreeContiguousAllocator&& other) noexcept {
    // Note: this is not thread-safe.
    full_buffer_ = other.full_buffer_;
    offset_.store(other.offset_.load(std::memory_order_relaxed), std::memory_order_relaxed);
    return *this;
  }

  std::span<const char> Allocate(size_t size) {
    size_t old_offset = offset_.load(std::memory_order_relaxed);

    while (true) {
      if (old_offset >= full_buffer_.size()) {
        if constexpr (EnableInfiniteLoop) {
          if (full_buffer_.empty()) {
            return {};
          }
          // Attempt to reset the offset to 0.
          if (!offset_.compare_exchange_weak(old_offset, 0, std::memory_order_release,
                                             std::memory_order_relaxed)) {
            // CAS failed, old_offset was updated by it. Retry loop.
            continue;
          }
          // We successfully reset the offset.
          old_offset = 0;
        } else {
          return {}; // End of buffer and not in infinite loop mode.
        }
      }

      // At this point, old_offset is within the buffer bounds.
      const size_t remaining_size = full_buffer_.size() - old_offset;
      size_t chunk_size = std::min(size, remaining_size);

      size_t scan_pos = old_offset + chunk_size;
      while (scan_pos < full_buffer_.size() && full_buffer_[scan_pos] != '\n') {
        scan_pos++;
      }

      if (scan_pos < full_buffer_.size()) {
        // Found a newline, include it in the chunk.
        scan_pos++;
      }

      const size_t new_offset = scan_pos;

      if (offset_.compare_exchange_weak(old_offset, new_offset, std::memory_order_release,
                                         std::memory_order_relaxed)) {
        // We successfully claimed a chunk of the buffer.
        return full_buffer_.subspan(old_offset, new_offset - old_offset);
      }
      // CAS failed, another thread allocated. old_offset is updated, so we just loop again.
    }
  }

  bool Empty() const {
    return offset_.load(std::memory_order_relaxed) >= full_buffer_.size();
  }

 private:
  Buffer full_buffer_;
  std::atomic<size_t> offset_{0};
};
